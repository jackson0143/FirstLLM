{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "# Import PyTorch's neural network module\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU parallel computing\n",
    "#### Torch uses GPU, whereas numpy uses CPU. lets compare the speed for a 4 dimensional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch time: 0.03594350814819336 seconds\n",
      "numpy time: 0.18724703788757324 seconds\n"
     ]
    }
   ],
   "source": [
    "torch_rand = torch.rand(100, 100,100,100).to(device)\n",
    "tord_rand_2 = torch.rand(100, 100,100,100).to(device)\n",
    "np_rand = np.random.rand(100, 100,100,100)\n",
    "np_rand_2 = np.random.rand(100, 100,100,100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = torch_rand @ tord_rand_2\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"torch time: {elapsed_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "rand=np.multiply(np_rand, np_rand_2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"numpy time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch time: 0.001001596450805664 seconds\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time= time.time()\n",
    "zeros=torch.zeros(1,1)\n",
    "end_time =  time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"torch time: {elapsed_time} seconds\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensor of randomly sampled indices based on probability\n",
    "\n",
    "#### Eg: index 0 has a 10%, index 1 has 20%, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 3, 0, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line creates a PyTorch tensor containing probabilities that sum to 1.0\n",
    "probabilities = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "# torch.multinomial performs weighted random sampling from the input tensor (probabilities)\n",
    "# - probabilities: input tensor containing probabilities for each element\n",
    "# - num_samples: number of samples to draw (10 in this case)\n",
    "# - replacement=True: allows sampling the same index multiple times\n",
    "#\n",
    "# For example, with probabilities [0.1, 0.2, 0.3, 0.4]:\n",
    "# - Index 0 (0.1) has 10% chance of being selected\n",
    "# - Index 1 (0.2) has 20% chance of being selected\n",
    "# - Index 2 (0.3) has 30% chance of being selected  \n",
    "# - Index 3 (0.4) has 40% chance of being selected\n",
    "\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "samples  # Returns tensor of 10 randomly sampled indices based on the probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3,4,5])\n",
    "out = torch.cat((tensor, tensor), dim=0)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Upper triangular matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower triangular matrix:\n",
      "tensor([[1, 0, 0],\n",
      "        [4, 5, 0],\n",
      "        [7, 8, 9]])\n",
      "Upper triangular matrix:\n",
      "tensor([[1, 2, 3],\n",
      "        [0, 5, 6],\n",
      "        [0, 0, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample matrix\n",
    "matrix = torch.tensor([[1, 2, 3], \n",
    "                      [4, 5, 6],\n",
    "                      [7, 8, 9]])\n",
    "\n",
    "# Get lower triangular part using tril()\n",
    "lower_triangular = torch.tril(matrix)\n",
    "print(\"Lower triangular matrix:\")\n",
    "print(lower_triangular)\n",
    "\n",
    "# Get upper triangular part using triu()\n",
    "upper_triangular = torch.triu(matrix)\n",
    "print(\"Upper triangular matrix:\")\n",
    "print(upper_triangular)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention masks used in attention mechanisms\n",
    "- Here we create a boolean mask to fill  the upper triangular (including diagonal) with -infinity\n",
    "\n",
    "- Lower keeps 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.create a 5x5 matrix of zeros\n",
    "base_matrix = torch.zeros(5,5)\n",
    "\n",
    "# 2. Create a 5x5 matrix of ones and get its lower triangular part\n",
    "ones_matrix = torch.ones(5,5)\n",
    "lower_tri_mask = torch.tril(ones_matrix)\n",
    "# This creates a mask where:\n",
    "# - Lower triangle (including diagonal) = 1\n",
    "# - Upper triangle = 0\n",
    "\n",
    "# 3. Create a boolean mask where True is for positions we want to fill\n",
    "mask = (lower_tri_mask == 0)\n",
    "# This makes the upper triangle True and lower triangle False\n",
    "\n",
    "# 4. Fill masked positions (upper triangle) with negative infinity\n",
    "out = base_matrix.masked_fill(mask, float('-inf'))\n",
    "# This means:\n",
    "# - Lower triangle keeps its zeros\n",
    "# - Upper triangle becomes negative infinity\n",
    "\n",
    "print(out)\n",
    "# The result is a 5x5 matrix where:\n",
    "# - Lower triangle (including diagonal) contains zeros\n",
    "# - Upper triangle contains negative infinity\n",
    "# This is commonly used in attention mechanisms to create attention masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponantials to the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Apply exponential function to the matrix\n",
    "# This transforms:\n",
    "# - zeros in lower triangle to ones (e^0 = 1)\n",
    "# - negative infinity in upper triangle to zeros (e^-inf â‰ˆ 0)\n",
    "result = torch.exp(out)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.zeros(2,3,4)\n",
    "out=input.transpose(1,2)\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking tensors, stacking dimensions\n",
    "- Appearantly tensors use 0-based indexing. \n",
    "- So this way the final tensor is a 2D tensor\n",
    "- dim=0 stacks vertically, each tensor becomes a row. dim=1 stacks horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create three 1D tensors\n",
    "tensor1 = torch.tensor([1,2,3])      # Shape: (3,)\n",
    "tensor2 = torch.tensor([4,5,6])      # Shape: (3,)\n",
    "tensor3 = torch.tensor([7,8,9])      # Shape: (3,)\n",
    "\n",
    "# Stack the tensors along dimension 0 (creating a new first dimension)\n",
    "# This combines the tensors into a single 2D tensor\n",
    "# Shape becomes: (3, 3) - 3 rows of 3 elements each\n",
    "stacked_tensor = torch.stack((tensor1, tensor2, tensor3), dim=0)\n",
    "\n",
    "# Display the resulting 2D tensor\n",
    "# Will show:\n",
    "# [[1, 2, 3],\n",
    "#  [4, 5, 6], \n",
    "#  [7, 8, 9]]\n",
    "stacked_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear combinations with random weights\n",
    "- First create a linear transformation layer of 3 inputs and 3 outputs\n",
    "- bias means no constant term\n",
    "#### $\\begin{bmatrix} 10 & 10 & 10 \\end{bmatrix} \\times \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{bmatrix} = \\begin{bmatrix} output_1 & output_2 & output_3 \\end{bmatrix}$\n",
    "##### Here each input is multiplied by a weight. All the products are then added together. The weights determine how much each input contributes to each output\n",
    "\n",
    "- Fundamental component of neural networks\n",
    "- Dimensions currently 3 but we can change to (3,5) to transform 3 features to 5\n",
    "- Helps learn complex patterns through multiple layers\n",
    "- Weights can be learned and optimise to minimise prediction errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.6242,  7.4097, -3.5206], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a sample input tensor with 3 values of 10.0\n",
    "sample = torch.tensor([10.,10.,10.])  # Input: [10, 10, 10]\n",
    "\n",
    "            # Output: [w11*10 + w12*10 + w13*10,\n",
    "            #          w21*10 + w22*10 + w23*10,\n",
    "            #          w31*10 + w32*10 + w33*10]\n",
    "\n",
    "\n",
    "# Create a linear layer that:\n",
    "# - Takes 3 input features\n",
    "# - Outputs 3 features \n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "# Pass the sample through the linear layer\n",
    "# This performs the operation: output = input * weights\n",
    "# Where weights is a 3x3 matrix initialized randomly\n",
    "out = linear(sample)\n",
    "\n",
    "# Print the output\n",
    "# This will show 3 values that are linear combinations \n",
    "# of the input [10,10,10] with the random weights\n",
    "print(out)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "- Takes numbers and converts them to probabilities that sum to 1\n",
    "- Makes larger values more significant\n",
    "- Helps with gradient flow in nn\n",
    "- e^x ensures exponential growth in values\n",
    "- Smooth, differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^x values:\n",
      "tensor([  2.7183,   7.3891,  20.0855,  54.5981, 148.4132])\n",
      "\n",
      "Sum of e^x values: tensor(233.2042)\n",
      "\n",
      "Softmax probabilities (e^x / sum(e^x)):\n",
      "tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364])\n",
      "\n",
      "Torch's softmax implementation:\n",
      "tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364])\n",
      "\n",
      "Sum of probabilities: 1.0\n",
      "\n",
      "Original values: tensor([1., 2., 3., 4., 5.])\n",
      "Final probabilities: tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a sample tensor of scores/logits\n",
    "sample = torch.tensor([1., 2., 3., 4., 5.])\n",
    "\n",
    "# Step 1: Calculate e^x for each value\n",
    "exp_values = torch.exp(sample)\n",
    "print(\"e^x values:\")\n",
    "print(exp_values)  # [e^1, e^2, e^3, e^4, e^5]\n",
    "\n",
    "# Step 2: Calculate sum of all e^x values\n",
    "sum_exp = exp_values.sum()\n",
    "print(\"\\nSum of e^x values:\", sum_exp)\n",
    "\n",
    "# Step 3: Divide each e^x by the sum to get probabilities\n",
    "softmax = exp_values / sum_exp\n",
    "print(\"\\nSoftmax probabilities (e^x / sum(e^x)):\")\n",
    "print(softmax)\n",
    "\n",
    "# Verify the calculation matches torch's implementation\n",
    "torch_softmax = F.softmax(sample, dim=0)\n",
    "print(\"\\nTorch's softmax implementation:\")\n",
    "print(torch_softmax)\n",
    "\n",
    "# Verify probabilities sum to 1\n",
    "print(\"\\nSum of probabilities:\", softmax.sum().item())\n",
    "\n",
    "# The larger the input value, the larger the output probability\n",
    "# This shows how softmax amplifies differences between values\n",
    "print(\"\\nOriginal values:\", sample)\n",
    "print(\"Final probabilities:\", softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8127,  0.3105, -0.7489],\n",
      "        [ 0.1444,  0.7144,  0.2271],\n",
      "        [-1.0443, -0.6564,  0.3397],\n",
      "        [-0.4447, -0.2291,  0.4641]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#initialise embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "\n",
    "#create a sample tensor of indices\n",
    "indices = torch.tensor([1,2,3,4])\n",
    "\n",
    "#pass indices to embedding layer\n",
    "embedded_output = embedding(indices)\n",
    "\n",
    "#print embedded output\n",
    "print(embedded_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4], [5,6]])\n",
    "b = torch.tensor([[7,8,9], [10,11,12]])\n",
    "\n",
    "res = torch.matmul(a,b)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying int and float don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1,(3,2)).float()\n",
    "float_32 = torch.rand(2,3)\n",
    "result = torch.matmul(int_64, float_32)\n",
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res = torch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-LLM2",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
